# ═══════════════════════════════════════════════════════════════
# RAGLOX v3.0 - Environment Variables
# Copy this file to .env and update with your values
# ═══════════════════════════════════════════════════════════════

# ═══════════════════════════════════════════════════════════════
# PostgreSQL Configuration
# ═══════════════════════════════════════════════════════════════
POSTGRES_USER=raglox
POSTGRES_PASSWORD=your_secure_password_here
POSTGRES_DB=raglox
DATABASE_URL=postgresql://raglox:your_secure_password_here@localhost:5432/raglox

# ═══════════════════════════════════════════════════════════════
# Redis Configuration
# ═══════════════════════════════════════════════════════════════
REDIS_URL=redis://localhost:6379/0
REDIS_PASSWORD=

# ═══════════════════════════════════════════════════════════════
# MinIO/S3 Configuration
# ═══════════════════════════════════════════════════════════════
MINIO_ACCESS_KEY=raglox_access
MINIO_SECRET_KEY=your_minio_secret_here
S3_ENDPOINT=http://localhost:9000
S3_BUCKET_PAYLOADS=payloads
S3_BUCKET_REPORTS=reports
S3_BUCKET_LOGS=logs
S3_BUCKET_EVIDENCE=evidence

# ═══════════════════════════════════════════════════════════════
# JWT Authentication
# ═══════════════════════════════════════════════════════════════
JWT_SECRET=your_jwt_secret_key_here_min_32_chars
JWT_ALGORITHM=HS256
JWT_EXPIRATION_HOURS=24

# ═══════════════════════════════════════════════════════════════
# API Configuration
# ═══════════════════════════════════════════════════════════════
API_HOST=0.0.0.0
API_PORT=8000
API_DEBUG=false
API_RELOAD=false

# ═══════════════════════════════════════════════════════════════
# Security Settings
# ═══════════════════════════════════════════════════════════════
# Encryption key for credentials (32 bytes, base64 encoded)
ENCRYPTION_KEY=your_base64_encoded_32_byte_key_here
CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# ═══════════════════════════════════════════════════════════════
# Logging
# ═══════════════════════════════════════════════════════════════
LOG_LEVEL=INFO
LOG_FORMAT=json

# ═══════════════════════════════════════════════════════════════
# Mission Defaults
# ═══════════════════════════════════════════════════════════════
MISSION_TIMEOUT_SECONDS=86400
MAX_CONCURRENT_MISSIONS=5
MAX_WORKERS_PER_SPECIALIST=5

# ═══════════════════════════════════════════════════════════════
# Development Settings (set to false in production)
# ═══════════════════════════════════════════════════════════════
DEV_MODE=false
ENABLE_PROFILING=false

# ═══════════════════════════════════════════════════════════════
# LLM Configuration (AI-Assisted Analysis)
# ═══════════════════════════════════════════════════════════════
# Enable/Disable LLM-assisted analysis
LLM_ENABLED=true

# Provider: openai, blackbox, local, mock
LLM_PROVIDER=blackbox

# ─────────────────────────────────────────────────────────────────
# API Keys for LLM Providers
# ─────────────────────────────────────────────────────────────────
# OpenAI API Key - Get your key from https://platform.openai.com/api-keys
# Required when LLM_PROVIDER=openai
OPENAI_API_KEY=

# Generic LLM API Key - Used by other providers (BlackboxAI, etc.)
# For BlackboxAI: Get your key from https://blackbox.ai
# This is also used as fallback if OPENAI_API_KEY is not set
LLM_API_KEY=

# API Base URL (optional - uses provider default if not set)
# BlackboxAI: https://api.blackbox.ai
# OpenAI: https://api.openai.com/v1
# Local Ollama: http://localhost:11434/v1
LLM_API_BASE=

# Model name
# BlackboxAI: gpt-4, gpt-3.5-turbo
# OpenAI: gpt-4o-mini, gpt-4-turbo
# Ollama: llama3.2:latest, mistral:latest
LLM_MODEL=gpt-4

# Generation parameters
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2048
LLM_TIMEOUT=60.0
LLM_MAX_RETRIES=3

# Enable fallback to rule-based analysis if LLM fails
LLM_FALLBACK_ENABLED=true

# ═══════════════════════════════════════════════════════════════
# LLM Safety Limits (IMPORTANT - Prevents runaway costs)
# ═══════════════════════════════════════════════════════════════
# Maximum cost limit in USD per mission
LLM_MAX_COST_LIMIT=2.0

# Maximum LLM API requests per day
LLM_DAILY_REQUESTS_LIMIT=100

# Maximum LLM API requests per single mission
LLM_MISSION_REQUESTS_LIMIT=20

# Estimated cost per 1000 tokens (for tracking)
LLM_COST_PER_1K_TOKENS=0.002

# Enable safety mode (stops on limit breach)
LLM_SAFETY_MODE=true

# ═══════════════════════════════════════════════════════════════
# Intel/OSINT - Elasticsearch Data Lake Configuration
# ═══════════════════════════════════════════════════════════════
# Enable Elasticsearch intel provider
INTEL_ELASTIC_ENABLED=false

# Elasticsearch URL for leaked data lake
INTEL_ELASTIC_URL=http://localhost:9200

# Authentication (choose one method):
# Option 1: API Key (recommended for production)
INTEL_ELASTIC_API_KEY=

# Option 2: Basic Auth
INTEL_ELASTIC_USERNAME=
INTEL_ELASTIC_PASSWORD=

# Index pattern for leaked credentials (supports wildcards)
INTEL_ELASTIC_INDEX=leaks-*

# Request timeout in seconds
INTEL_ELASTIC_TIMEOUT=30

# Maximum retries for connection failures
INTEL_ELASTIC_MAX_RETRIES=3

# SSL/TLS settings
INTEL_ELASTIC_VERIFY_CERTS=true
INTEL_ELASTIC_CA_CERTS=

# ═══════════════════════════════════════════════════════════════
# Intel - Local File Search Configuration
# ═══════════════════════════════════════════════════════════════
# Directory for local breach data files
INTEL_FILE_DATA_DIR=./data/breach_data

# Priority boost for intel credentials over brute force (0.0-0.5)
# Higher value = stronger preference for leaked credentials
INTEL_CREDENTIAL_PRIORITY_BOOST=0.3
